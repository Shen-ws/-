,title,conference,year,authors,abstract,pdf_path,pdf_url,code_url,relevant,tran_flag
0,PlaneDepth: Self-Supervised Depth Estimation via Orthogonal Planes,CVPR,2023,"Ruoyu Wang, Zehao Yu, Shenghua Gao","Multiple near frontal-parallel planes based depth representation demonstrated impressive results in self-supervised monocular depth estimation (MDE). Whereas, such a representation would cause the discontinuity of the ground as it is perpendicular to the frontal-parallel planes, which is detrimental to the identification of drivable space in autonomous driving. In this paper, we propose the PlaneDepth, a novel orthogonal planes based presentation, including vertical planes and ground planes. PlaneDepth estimates the depth distribution using a Laplacian Mixture Model based on orthogonal planes for an input image. These planes are used to synthesize a reference view to provide the self-supervision signal. Further, we find that the widely used resizing and cropping data augmentation breaks the orthogonality assumptions, leading to inferior plane predictions. We address this problem by explicitly constructing the resizing cropping transformation to rectify the predefined planes and predicted camera pose. Moreover, we propose an augmented self-distillation loss supervised with a bilateral occlusion mask to boost the robustness of orthogonal planes representation for occlusions. Thanks to our orthogonal planes representation, we can extract the ground plane in an unsupervised manner, which is important for autonomous driving. Extensive experiments on the KITTI dataset demonstrate the effectiveness and efficiency of our method. The code is available at https://github.com/svip-lab/PlaneDepth.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_PlaneDepth_Self-Supervised_Depth_Estimation_via_Orthogonal_Planes_CVPR_2023_paper.pdf,https://github.com/svip-lab/PlaneDepth,3,本文提出了一种新的基于正交平面的深度表示方法――PlaneDepth，包括垂直平面和地面平面。PlaneDepth使用基于正交平面的拉普拉斯混合模型来估计输入图像的深度分布，并利用这些平面合成参考视图以提供自我监督信号。此外，作者发现常用 的缩放和裁剪数据增强会破坏正交性假设，导致平面预测效果不佳。作者通过显式构造缩放裁剪变换来矫正预定义平面和预测相机姿态，解决了这个问题。此外，作者还提出了一种增强的自蒸馏损失，通过双边遮挡掩码来提高正交平面表示对遮挡的鲁棒性。由于正交平 面表示，我们可以无监督地提取地面平面，这对于自动驾驶非常重要。在KITTI数据集上的大量实验表明了我们方法的有效
1,OpenGait: Revisiting Gait Recognition Towards Better Practicality,CVPR,2023,"Chao Fan, Junhao Liang, Chuanfu Shen, Saihui Hou, Yongzhen Huang, Shiqi Yu","Gait recognition is one of the most critical long-distance identification technologies and increasingly gains popularity in both research and industry communities. Despite the significant progress made in indoor datasets, much evidence shows that gait recognition techniques perform poorly in the wild. More importantly, we also find that some conclusions drawn from indoor datasets cannot be generalized to real applications. Therefore, the primary goal of this paper is to present a comprehensive benchmark study for better practicality rather than only a particular model for better performance. To this end, we first develop a flexible and efficient gait recognition codebase named OpenGait. Based on OpenGait, we deeply revisit the recent development of gait recognition by re-conducting the ablative experiments. Encouragingly,we detect some unperfect parts of certain prior woks, as well as new insights. Inspired by these discoveries, we develop a structurally simple, empirically powerful, and practically robust baseline model, GaitBase. Experimentally, we comprehensively compare GaitBase with many current gait recognition methods on multiple public datasets, and the results reflect that GaitBase achieves significantly strong performance in most cases regardless of indoor or outdoor situations. Code is available at https://github.com/ShiqiYu/OpenGait.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Fan_OpenGait_Revisiting_Gait_Recognition_Towards_Better_Practicality_CVPR_2023_paper.pdf,https://github.com/ShiqiYu/OpenGait,3,本文介绍了步态识别技术在长距离身份识别中的重要性，并指出尽管在室内数据集方面取得了显著进展，但步态识别技术在野外表现不佳。因此，本文的主要目标是提出一个全面的基准研究，以实现更好的实用性，而不仅仅是为了更好的性能而提出 一个特定的模型。为此，作者首先开发了一个灵活高效的步态识别代码库OpenGait，并在此基础上重新进行了削减实验，发现了一些之前工作的不足之处和新的见解。基于这些发现，作者开发了一个结构简单、实证强大、实用稳健的基线模型GaitBase，并在多个公共数 据集上全面比较了GaitBase与许多当前的步态识别方法，结果表明GaitBase在大多数情况下都取得了显著的强劲表现，无论是
2,An In-Depth Exploration of Person Re-Identification and Gait Recognition in Cloth-Changing Conditions,CVPR,2023,"Weijia Li, Saihui Hou, Chunjie Zhang, Chunshui Cao, Xu Liu, Yongzhen Huang, Yao Zhao","The target of person re-identification (ReID) and gait recognition is consistent, that is to match the target pedestrian under surveillance cameras. For the cloth-changing problem, video-based ReID is rarely studied due to the lack of a suitable cloth-changing benchmark, and gait recognition is often researched under controlled conditions. To tackle this problem, we propose a Cloth-Changing benchmark for Person re-identification and Gait recognition (CCPG). It is a cloth-changing dataset, and there are several highlights in CCPG, (1) it provides 200 identities and over 16K sequences are captured indoors and outdoors, (2) each identity has seven different cloth-changing statuses, which is hardly seen in previous datasets, (3) RGB and silhouettes version data are both available for research purposes. Moreover, aiming to investigate the cloth-changing problem systematically, comprehensive experiments are conducted on video-based ReID and gait recognition methods. The experimental results demonstrate the superiority of ReID and gait recognition separately in different cloth-changing conditions and suggest that gait recognition is a potential solution for addressing the cloth-changing problem. Our dataset will be available at https://github.com/BNU-IVC/CCPG.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_An_In-Depth_Exploration_of_Person_Re-Identification_and_Gait_Recognition_in_CVPR_2023_paper.pdf,https://github.com/BNU-IVC/CCPG,3,本文介绍了人物再识别（ReID）和步态识别的共同目标，即在监控摄像头下匹配目标行人。由于缺乏适合的衣服更换基准，视频型ReID很少被研究，而步态识别通常在受控条件下进行研究。为了解决这个问题，作者提出了一个用于人物再识别和步态 识别的衣服更换基准（CCPG）。它是一个衣服更换数据集，其中有几个亮点：（1）提供了200个身份和超过16K个室内外序列，（2）每个身份有七种不同的衣服更换状态，在以前的数据集中很少见，（3）RGB和轮廓版本数据都可用于研究目的。此外，为了系统地研究衣 服更换问题，作者对视频型ReID和步态识别方法进行了全面的实验。实验结果表明，ReID和步态识别在不同的
3,Towards Modality-Agnostic Person Re-Identification With Descriptive Query,CVPR,2023,"Cuiqun Chen, Mang Ye, Ding Jiang","Person re-identification (ReID) with descriptive query (text or sketch) provides an important supplement for general image-image paradigms, which is usually studied in a single cross-modality matching manner, e.g., text-to-image or sketch-to-photo. However, without a camera-captured photo query, it is uncertain whether the text or sketch is available or not in practical scenarios. This motivates us to study a new and challenging modality-agnostic person re-identification problem. Towards this goal, we propose a unified person re-identification (UNIReID) architecture that can effectively adapt to cross-modality and multi-modality tasks. Specifically, UNIReID incorporates a simple dual-encoder with task-specific modality learning to mine and fuse visual and textual modality information. To deal with the imbalanced training problem of different tasks in UNIReID, we propose a task-aware dynamic training strategy in terms of task difficulty, adaptively adjusting the training focus. Besides, we construct three multi-modal ReID datasets by collecting the corresponding sketches from photos to support this challenging task. The experimental results on three multi-modal ReID datasets show that our UNIReID greatly improves the retrieval accuracy and generalization ability on different tasks and unseen scenarios.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Towards_Modality-Agnostic_Person_Re-Identification_With_Descriptive_Query_CVPR_2023_paper.pdf,,3,本文介绍了一种新的、具有挑战性的跨模态人物再识别问题，即在没有相机拍摄的照片查询的情况下，如何有效地利用文本或素描进行人物再识别。为了解决这个问题，作者提出了一种统一的人物再识别（UNIReID）架构，该架构可以有效地适应跨模态和多模态任务。UNIReID采用了一个简单的双编码器和任务特定的模态学习来挖掘和融合视觉和文本模态信息。为了解决UNIReID中不同任务的不平衡训练问题，作者提出了一种任务感知的动态训练策略，根据任务难度自适应地调整训练重点。此外，作者构建了三个多 模态ReID数据集，通过收集相应的素描来支持这个具有挑战性的任务。实验结果表明，UNIReID极大地提高了不同任务和
4,Visual Recognition-Driven Image Restoration for Multiple Degradation With Intrinsic Semantics Recovery,CVPR,2023,"Zizheng Yang, Jie Huang, Jiahao Chang, Man Zhou, Hu Yu, Jinghao Zhang, Feng Zhao","Deep image recognition models suffer a significant performance drop when applied to low-quality images since they are trained on high-quality images. Although many studies have investigated to solve the issue through image restoration or domain adaptation, the former focuses on visual quality rather than recognition quality, while the latter requires semantic annotations for task-specific training. In this paper, to address more practical scenarios, we propose a Visual Recognition-Driven Image Restoration network for multiple degradation, dubbed VRD-IR, to recover high-quality images from various unknown corruption types from the perspective of visual recognition within one model. Concretely, we harmonize the semantic representations of diverse degraded images into a unified space in a dynamic manner, and then optimize them towards intrinsic semantics recovery. Moreover, a prior-ascribing optimization strategy is introduced to encourage VRD-IR to couple with various downstream recognition tasks better. Our VRD-IR is corruption- and recognition-agnostic, and can be inserted into various recognition tasks directly as an image enhancement module. Extensive experiments on multiple image distortions demonstrate that our VRD-IR surpasses existing image restoration methods and show superior performance on diverse high-level tasks, including classification, detection, and person re-identification.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Visual_Recognition-Driven_Image_Restoration_for_Multiple_Degradation_With_Intrinsic_Semantics_CVPR_2023_paper.pdf,,3,本文提出了一种基于视觉识别的图像恢复网络VRD-IR，旨在从视觉识别的角度恢复各种未知损坏类型的高质量图像。该网络将不同降级图像的语义表示动态地协调到一个统一的空间中，并优化它们以实现内在语义恢复。此外，引入了一种先验归因优 化策略，以更好地与各种下游识别任务相结合。VRD-IR不受损坏和识别的影响，可以直接插入各种识别任务作为图像增强模块。多种图像失真的广泛实验表明，VRD-IR超越了现有的图像恢复方法，并在分类、检测和人员重新识别等各种高级任务中表现出卓越的性能。
5,Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning,CVPR,2023,"Zesen Wu, Mang Ye","Unsupervised visible-infrared person re-identification is a challenging task due to the large modality gap and the unavailability of cross-modality correspondences. Cross-modality correspondences are very crucial to bridge the modality gap. Some existing works try to mine cross-modality correspondences, but they focus only on local information. They do not fully exploit the global relationship across identities, thus limiting the quality of the mined correspondences. Worse still, the number of clusters of the two modalities is often inconsistent, exacerbating the unreliability of the generated correspondences. In response, we devise a Progressive Graph Matching method to globally mine cross-modality correspondences under cluster imbalance scenarios. PGM formulates correspondences mining as a graph matching process and considers the global information by minimizing the global matching cost, where the matching cost measures the dissimilarity of clusters. Besides, PGM adopts a progressive strategy to address the imbalance issue with multiple dynamic matching processes. Based on PGM, we design an Alternate Cross Contrastive Learning (ACCL) module to reduce the modality gap with the mined cross-modality correspondences, while mitigating the effect of noise in correspondences through an alternate scheme. Extensive experiments demonstrate the reliability of the generated correspondences and the effectiveness of our method.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_Unsupervised_Visible-Infrared_Person_Re-Identification_via_Progressive_Graph_Matching_and_Alternate_CVPR_2023_paper.pdf,,3,本文介绍了一种无监督的可见-红外人员再识别方法，该方法面临着模态差异大和跨模态对应关系不可用的挑战。跨模态对应关系对于弥合模态差距非常关键。一些现有的工作尝试挖掘跨模态对应关系，但它们仅关注局部信息。它们没有充分利用跨身份的全局关系，从而限制了挖掘到的对应关系的质量。更糟糕的是，两种模态的聚类数量通常不一致，加剧了生成的对应关系的不可靠性。为了解决这个问题，我们设计了一种渐进式图匹配方法，在聚类不平衡的情况下全局挖掘跨模态对应关系。PGM将对应关系挖掘形式化为图匹配过程，并通过最小化全局匹配成本来考虑全局信息，其中匹配成本衡量
6,Clothing-Change Feature Augmentation for Person Re-Identification,CVPR,2023,"Ke Han, Shaogang Gong, Yan Huang, Liang Wang, Tieniu Tan","Clothing-change person re-identification (CC Re-ID) aims to match the same person who changes clothes across cameras. Current methods are usually limited by the insufficient number and variation of clothing in training data, e.g. each person only has 2 outfits in the PRCC dataset. In this work, we propose a novel Clothing-Change Feature Augmentation (CCFA) model for CC Re-ID to largely expand clothing-change data in the feature space rather than visual image space. It automatically models the feature distribution expansion that reflects a person's clothing colour and texture variations to augment model training. Specifically, to formulate meaningful clothing variations in the feature space, our method first estimates a clothing-change normal distribution with intra-ID cross-clothing variances. Then an augmentation generator learns to follow the estimated distribution to augment plausible clothing-change features. The augmented features are guaranteed to maximise the change of clothing and minimise the change of identity properties by adversarial learning to assure the effectiveness. Such augmentation is performed iteratively with an ID-correlated augmentation strategy to increase intra-ID clothing variations and reduce inter-ID clothing variations, enforcing the Re-ID model to learn clothing-independent features inherently. Extensive experiments demonstrate the effectiveness of our method with state-of-the-art results on CC Re-ID datasets.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Han_Clothing-Change_Feature_Augmentation_for_Person_Re-Identification_CVPR_2023_paper.pdf,,3, 本文介绍了一种新的服装变换特征增强模型，用于解决服装变换人员再识别问题。当前的方法通常受到训练数据中服装数量和变化的限制，例如PRCC数据集中每个人只有2套服装。该模型通过在特征空间而非视觉图像空间中扩展服装变化数据，自动建模反映人的服装颜色和纹理变化的特征分布扩展来增强模型训练。具体而言，该方法首先估计服装变化正态分布，然后通过增强生成器学习遵循估计分布以增强可信服装变化特征。增强特征通过对抗学习来保证最大化服装变化和最小化身份属性变化，以确保有效性。这 种增强是通过ID相关的增强策略进行迭代执行的，以增加内部ID服装变化并减少ID间服装变化，从而强制Re-ID模型在本
7,Samples With Low Loss Curvature Improve Data Efficiency,CVPR,2023,"Isha Garg, Kaushik Roy","In this paper, we study the second order properties of the loss of trained deep neural networks with respect to the training data points to understand the curvature of the loss surface in the vicinity of these points. We find that there is an unexpected concentration of samples with very low curvature. We note that these low curvature samples are largely consistent across completely different architectures, and identifiable in the early epochs of training. We show that the curvature relates to the 'cleanliness' of the data points, with low curvatures samples corresponding to clean, higher clarity samples, representative of their category. Alternatively, high curvature samples are often occluded, have conflicting features and visually atypical of their category. Armed with this insight, we introduce SLo-Curves, a novel coreset identification and training algorithm. SLo-curves identifies the samples with low curvatures as being more data-efficient and trains on them with an additional regularizer that penalizes high curvature of the loss surface in their vicinity. We demonstrate the efficacy of SLo-Curves on CIFAR-10 and CIFAR-100 datasets, where it outperforms state of the art coreset selection methods at small coreset sizes by up to 9%. The identified coresets generalize across architectures, and hence can be pre-computed to generate condensed versions of datasets for use in downstream tasks.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Garg_Samples_With_Low_Loss_Curvature_Improve_Data_Efficiency_CVPR_2023_paper.pdf,,3,本文研究了训练深度神经网络时损失函数对训练数据点的二阶性质，以了解这些点附近的损失曲面的曲率。研究发现，存在许多具有非常低曲率的样本，这是出乎意料的。这些低曲率样本在完全不同的架构中大多是一致的，并且在训练的早期阶段就 可以识别出来。研究表明，曲率与数据点的“清洁度”有关，低曲率样本对应于干净、更具代表性的高清晰度样本。相反，高曲率样本通常被遮挡，具有冲突的特征，并且在视觉上不典型。基于这一发现，本文提出了一种新的核心集识别和训练算法SLo-Curves。SLo-Curves将低曲率样本识别为更具数据效率，并在其周围
8,Event-Guided Person Re-Identification via Sparse-Dense Complementary Learning,CVPR,2023,"Chengzhi Cao, Xueyang Fu, Hongjian Liu, Yukun Huang, Kunyu Wang, Jiebo Luo, Zheng-Jun Zha","Video-based person re-identification (Re-ID) is a prominent computer vision topic due to its wide range of video surveillance applications. Most existing methods utilize spatial and temporal correlations in frame sequences to obtain discriminative person features. However, inevitable degradations, e.g., motion blur contained in frames often cause ambiguity texture noise and temporal disturbance, leading to the loss of identity-discriminating cues. Recently, a new bio-inspired sensor called event camera, which can asynchronously record intensity changes, brings new vitality to the Re-ID task. With the microsecond resolution and low latency, event cameras can accurately capture the movements of pedestrians even in the aforementioned degraded environments. Inspired by the properties of event cameras, in this work, we propose a Sparse-Dense Complementary Learning Framework, which effectively extracts identity features by fully exploiting the complementary information of dense frames and sparse events. Specifically, for frames, we build a CNN-based module to aggregate the dense features of pedestrian appearance step-by-step, while for event streams, we design a bio-inspired spiking neural backbone, which encodes event signals into sparse feature maps in a spiking form, to present the dynamic motion cues of pedestrians. Finally, a cross feature alignment module is constructed to complementarily fuse motion information from events and appearance cues from frames to enhance identity representation learning. Experiments on several benchmarks show that by employing events and SNN into Re-ID, our method significantly outperforms competitive methods.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Cao_Event-Guided_Person_Re-Identification_via_Sparse-Dense_Complementary_Learning_CVPR_2023_paper.pdf,,3,视频人物再识别（Re-ID）是计算机视觉领域的一个重要话题，因为它在视频监控应用中有广泛的应用。大多数现有的方法利用帧序列中的空间和时间相关性来获取具有区分性的人物特征。然而，不可避免的退化，例如包含在帧中的运动模糊经常导致纹理噪声和时间干扰，从而导致失去身份区分线索。最近，一种新的生物启发式传感器称为事件相机，它可以异步记录强度变化，为Re-ID任务带来了新的活力。具有微秒分辨率和低延迟的事件相机可以准确地捕捉行人的运动，即使在上述退化环境中也是如此。受事件相机的特性启发，本文提出了一种稀疏-密集互补学习框架，通过充分利用密集帧和稀疏事件的互补
9,PartMix: Regularization Strategy To Learn Part Discovery for Visible-Infrared Person Re-Identification,CVPR,2023,"Minsu Kim, Seungryong Kim, Jungin Park, Seongheon Park, Kwanghoon Sohn","Modern data augmentation using a mixture-based technique can regularize the models from overfitting to the training data in various computer vision applications, but a proper data augmentation technique tailored for the part-based Visible-Infrared person Re-IDentification (VI-ReID) models remains unexplored. In this paper, we present a novel data augmentation technique, dubbed PartMix, that synthesizes the augmented samples by mixing the part descriptors across the modalities to improve the performance of part-based VI-ReID models. Especially, we synthesize the positive and negative samples within the same and across different identities and regularize the backbone model through contrastive learning. In addition, we also present an entropy-based mining strategy to weaken the adverse impact of unreliable positive and negative samples. When incorporated into existing part-based VI-ReID model, PartMix consistently boosts the performance. We conduct experiments to demonstrate the effectiveness of our PartMix over the existing VI-ReID methods and provide ablation studies.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Kim_PartMix_Regularization_Strategy_To_Learn_Part_Discovery_for_Visible-Infrared_Person_CVPR_2023_paper.pdf,,3,本文提出了一种新的数据增强技术 PartMix，通过混合不同模态的部分描述符来合成增强样本，从而改善基于部分的可见光红外人员再识别模型的性能。特别地，我们在同一身份和不同身份之间合成正负样本，并通过对比学习来规范化骨干模型。此 外，我们还提出了一种基于熵的挖掘策略，以减弱不可靠正负样本的不良影响。将 PartMix 应用于现有的基于部分的 VI-ReID 模型中，可以持续提高性能。我们进行了实验，证明了 PartMix 在现有 VI-ReID 方法上的有效性，并提供了消融研究。
10,UniHCP: A Unified Model for Human-Centric Perceptions,CVPR,2023,"Yuanzheng Ci, Yizhou Wang, Meilin Chen, Shixiang Tang, Lei Bai, Feng Zhu, Rui Zhao, Fengwei Yu, Donglian Qi, Wanli Ouyang","Human-centric perceptions (e.g., pose estimation, human parsing, pedestrian detection, person re-identification, etc.) play a key role in industrial applications of visual models. While specific human-centric tasks have their own relevant semantic aspect to focus on, they also share the same underlying semantic structure of the human body. However, few works have attempted to exploit such homogeneity and design a general-propose model for human-centric tasks. In this work, we revisit a broad range of human-centric tasks and unify them in a minimalist manner. We propose UniHCP, a Unified Model for Human-Centric Perceptions, which unifies a wide range of human-centric tasks in a simplified end-to-end manner with the plain vision transformer architecture. With large-scale joint training on 33 humancentric datasets, UniHCP can outperform strong baselines on several in-domain and downstream tasks by direct evaluation. When adapted to a specific task, UniHCP achieves new SOTAs on a wide range of human-centric tasks, e.g., 69.8 mIoU on CIHP for human parsing, 86.18 mA on PA-100K for attribute prediction, 90.3 mAP on Market1501 for ReID, and 85.8 JI on CrowdHuman for pedestrian detection, performing better than specialized models tailored for each task. The code and pretrained model are available at https://github.com/OpenGVLab/UniHCP.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Ci_UniHCP_A_Unified_Model_for_Human-Centric_Perceptions_CVPR_2023_paper.pdf,https://github.com/OpenGVLab/UniHCP,3,本文介绍了人类中心感知在视觉模型的工业应用中发挥的关键作用。虽然特定的人类中心任务有其自己的相关语义方面需要关注，但它们也共享人体的相同基础语义结构。然而，很少有研究尝试利用这种同质性并为人类中心任务设计通用模型。本文 重新审视了广泛的人类中心任务，并以简化的端到端方式统一了它们。我们提出了UniHCP，一种统一的人类中心感知模型，它使用简单的视觉变换器架构简化了广泛的人类中心任务。通过在33个人类中心数据集上进行大规模联合训练，UniHCP可以通过直接评估在几个领 域内和下游任务上优于强基线。当适应于特定任务时，UniHCP在广泛的人类中心任务上实现了新的SOTAs，例如，在CIHP上进行人类解析时的69.8 mIoU
11,StyleGene: Crossover and Mutation of Region-Level Facial Genes for Kinship Face Synthesis,CVPR,2023,"Hao Li, Xianxu Hou, Zepeng Huang, Linlin Shen","High-fidelity kinship face synthesis has many potential applications, such as kinship verification, missing child identification, and social media analysis. However, it is challenging to synthesize high-quality descendant faces with genetic relations due to the lack of large-scale, high-quality annotated kinship data. This paper proposes RFG (Region-level Facial Gene) extraction framework to address this issue. We propose to use IGE (Image-based Gene Encoder), LGE (Latent-based Gene Encoder) and Gene Decoder to learn the RFGs of a given face image, and the relationships between RFGs and the latent space of StyleGAN2. As cycle-like losses are designed to measure the L_2 distances between the output of Gene Decoder and image encoder, and that between the output of LGE and IGE, only face images are required to train our framework, i.e. no paired kinship face data is required. Based upon the proposed RFGs, a crossover and mutation module is further designed to inherit the facial parts of parents. A Gene Pool has also been used to introduce the variations into the mutation of RFGs. The diversity of the faces of descendants can thus be significantly increased. Qualitative, quantitative, and subjective experiments on FIW, TSKinFace, and FF-Databases clearly show that the quality and diversity of kinship faces generated by our approach are much better than the existing state-of-the-art methods.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Li_StyleGene_Crossover_and_Mutation_of_Region-Level_Facial_Genes_for_Kinship_CVPR_2023_paper.pdf,,3,本文提出了一种名为RFG（区域级面部基因）提取框架的方法，以解决合成高质量后代面孔的挑战。该方法使用基于图像和基于潜在空间的基因编码器以及基因解码器来学习给定面部图像的RFG，并将RFG与StyleGAN2的潜在空间之间的关系进行了研究 。通过设计类似于循环的损失来衡量基因解码器和图像编码器之间的输出以及LGE和IGE之间的输出之间的L_2距离，我们的框架只需要面部图像进行训练，即不需要成对的亲缘面孔数据。基于所提出的RFG，还进一步设计了交叉和突变模块来继承父母的面部部分，并使用 基因池来引入变异的变化。在FIW、TSKinFace和FF数据库上进行的定性、定量和主观实验清楚地表明，我们的方法生成的亲
12,PHA: Patch-Wise High-Frequency Augmentation for Transformer-Based Person Re-Identification,CVPR,2023,"Guiwei Zhang, Yongfei Zhang, Tianyu Zhang, Bo Li, Shiliang Pu","Although recent studies empirically show that injecting Convolutional Neural Networks (CNNs) into Vision Transformers (ViTs) can improve the performance of person re-identification, the rationale behind it remains elusive. From a frequency perspective, we reveal that ViTs perform worse than CNNs in preserving key high-frequency components (e.g, clothes texture details) since high-frequency components are inevitably diluted by low-frequency ones due to the intrinsic Self-Attention within ViTs. To remedy such inadequacy of the ViT, we propose a Patch-wise High-frequency Augmentation (PHA) method with two core designs. First, to enhance the feature representation ability of high-frequency components, we split patches with high-frequency components by the Discrete Haar Wavelet Transform, then empower the ViT to take the split patches as auxiliary input. Second, to prevent high-frequency components from being diluted by low-frequency ones when taking the entire sequence as input during network optimization, we propose a novel patch-wise contrastive loss. From the view of gradient optimization, it acts as an implicit augmentation to improve the representation ability of key high-frequency components. This benefits the ViT to capture key high-frequency components to extract discriminative person representations. PHA is necessary during training and can be removed during inference, without bringing extra complexity. Extensive experiments on widely-used ReID datasets validate the effectiveness of our method.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_PHA_Patch-Wise_High-Frequency_Augmentation_for_Transformer-Based_Person_Re-Identification_CVPR_2023_paper.pdf,,3,最近的研究表明，将卷积神经网络（CNN）注入视觉变换器（ViT）可以提高人员再识别的性能，但其背后的原理仍然不清楚。从频率的角度来看，我们发现ViT在保留关键高频组件（例如服装纹理细节）方面比CNN表现更差，因为由于ViT内在的自我关注，高频组件不可避免地被低频组件稀释。为了解决ViT的这种不足，我们提出了一种基于补丁的高频增强（PHA）方法，具有两个核心设计。首先，为了增强高频组件的特征表示能力，我们通过离散哈尔小波变换将高频组件的补丁分割，然后使ViT将分割的补丁作为辅助输入。其次，在网络优化期间将整个序列作为输入时，为了防止高频组件被低频组件稀释，我们提出
13,Beyond Appearance: A Semantic Controllable Self-Supervised Learning Framework for Human-Centric Visual Tasks,CVPR,2023,"Weihua Chen, Xianzhe Xu, Jian Jia, Hao Luo, Yaohua Wang, Fan Wang, Rong Jin, Xiuyu Sun","Human-centric visual tasks have attracted increasing research attention due to their widespread applications. In this paper, we aim to learn a general human representation from massive unlabeled human images which can benefit downstream human-centric tasks to the maximum extent. We call this method SOLIDER, a Semantic cOntrollable seLf-supervIseD lEaRning framework. Unlike the existing self-supervised learning methods, prior knowledge from human images is utilized in SOLIDER to build pseudo semantic labels and import more semantic information into the learned representation. Meanwhile, we note that different downstream tasks always require different ratios of semantic information and appearance information. For example, human parsing requires more semantic information, while person re-identification needs more appearance information for identification purpose. So a single learned representation cannot fit for all requirements. To solve this problem, SOLIDER introduces a conditional network with a semantic controller. After the model is trained, users can send values to the controller to produce representations with different ratios of semantic information, which can fit different needs of downstream tasks. Finally, SOLIDER is verified on six downstream human-centric visual tasks. It outperforms state of the arts and builds new baselines for these tasks. The code is released in https://github.com/tinyvision/SOLIDER.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Beyond_Appearance_A_Semantic_Controllable_Self-Supervised_Learning_Framework_for_Human-Centric_CVPR_2023_paper.pdf,https://github.com/tinyvision/SOLIDER,3,本文旨在从大量未标记的人类图像中学习一种通用的人类表示方法，以最大程度地受益于下游的以人为中心的任务。我们称之为 SOLIDER，即语义可控的自我监督学习框架。与现有的自我监督学习方法不同，SOLIDER 利用人类图像的先验知识构建伪 语义标签，并将更多的语义信息导入到学习到的表示中。同时，我们注意到不同的下游任务总是需要不同比例的语义信息和外观信息。例如，人体分割需要更多的语义信息，而人员重新识别需要更多的外观信息以进行识别。因此，单一的学习表示不能适用于所有要求。 为了解决这个问题，SOLIDER 引入了一个带有语义控制器的条件网络。在模型训练完成后，用户可以向控制器发送值，以产生具有不同比例的语义信息的表示，以适应下游任务的不同需求。
14,Similarity Metric Learning for RGB-Infrared Group Re-Identification,CVPR,2023,"Jianghao Xiong, Jianhuang Lai","Group re-identification (G-ReID) aims to re-identify a group of people that is observed from non-overlapping camera systems. The existing literature has mainly addressed RGB-based problems, but RGB-infrared (RGB-IR) cross-modality matching problem has not been studied yet. In this paper, we propose a metric learning method Closest Permutation Matching (CPM) for RGB-IR G-ReID. We model each group as a set of single-person features which are extracted by MPANet, then we propose the metric Closest Permutation Distance (CPD) to measure the similarity between two sets of features. CPD is invariant with order changes of group members so that it solves the layout change problem in G-ReID. Furthermore, we introduce the problem of G-ReID without person labels. In the weak-supervised case, we design the Relation-aware Module (RAM) that exploits visual context and relations among group members to produce a modality-invariant order of features in each group, with which group member features within a set can be sorted to form a robust group representation against modality change. To support the study on RGB-IR G-ReID, we construct a new large-scale RGB-IR G-ReID dataset CM-Group. The dataset contains 15,440 RGB images and 15,506 infrared images of 427 groups and 1,013 identities. Extensive experiments on the new dataset demonstrate the effectiveness of the proposed models and the complexity of CM-Group. The code and dataset are available at: https://github.com/WhollyOat/CM-Group.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Similarity_Metric_Learning_for_RGB-Infrared_Group_Re-Identification_CVPR_2023_paper.pdf,https://github.com/WhollyOat/CM-Group,3,本文提出了一种度量学习方法 Closest Permutation Matching (CPM) 用于 RGB-IR G-ReID，解决了 RGB-IR 跨模态匹配问题。作者将每个群体建模为一组由 MPANet 提取的单人特征，并提出了 Closest Permutation Distance (CPD) 度量来衡量两 组特征之间的相似性。CPD 不受群体成员顺序变化的影响，从而解决了 G-ReID 中的布局变化问题。此外，作者还介绍了无人员标签的 G-ReID 问题。在弱监督情况下，作者设计了 Relation-aware Module (RAM) 来利用视觉上下文和群体成员之间的关系，产生每个群体中特征的模态不变顺序，从而可以对一组内的群体成员特征进行排序，形成针对模态变化的强大群体表示。为了支持 RGB-IR G-ReID 的研究，作者构建了一个新的大规模 RGB-IR G-ReID 数据集 CM-Group
15,CRAFT: Concept Recursive Activation FacTorization for Explainability,CVPR,2023,"Thomas Fel, Agustin Picard, Louis B茅thune, Thibaut Boissin, David Vigouroux, Julien Colin, R茅mi Cad猫ne, Thomas Serre","Attribution methods are a popular class of explainability methods that use heatmaps to depict the most important areas of an image that drive a model decision. Nevertheless, recent work has shown that these methods have limited utility in practice, presumably because they only highlight the most salient parts of an image (i.e., ""where"" the model looked) and do not communicate any information about ""what"" the model saw at those locations. In this work, we try to fill in this gap with Craft -- a novel approach to identify both ""what"" and ""where"" by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps. We conduct both human and computer vision experiments to demonstrate the benefits of the proposed approach. We show that our recursive decomposition generates meaningful and accurate concepts and that the proposed concept importance estimation technique is more faithful to the model than previous methods. When evaluating the usefulness of the method for human experimenters on the utility benchmark, we find that our approach significantly improves on two of the three test scenarios (while none of the current methods including ours help on the third). Overall, our study suggests that, while much work remains toward the development of general explainability methods that are useful in practical scenarios, the identification of meaningful concepts at the proper level of granularity yields useful and complementary information beyond that afforded by attribution methods.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Fel_CRAFT_Concept_Recursive_Activation_FacTorization_for_Explainability_CVPR_2023_paper.pdf,,3,本文介绍了一种名为Craft的新方法，用于生成基于概念的解释，以填补现有的解释方法的局限性。传统的解释方法只能突出图像中最显著的部分，而无法传达模型在这些位置看到的内容。Craft通过生成概念来识别“何时”和“何地”，并引入了三 个新的自动概念提取技术：（i）递归策略来检测和分解概念，（ii）使用Sobol指数更准确地估计概念重要性的新方法，以及（iii）使用隐式微分来解锁概念归因图。作者进行了人类和计算机视觉实验，证明了该方法的优点。研究表明，识别适当粒度的有意义的概念可以提供有用的补充信息，超越了传统的归因方法。
16,Good Is Bad: Causality Inspired Cloth-Debiasing for Cloth-Changing Person Re-Identification,CVPR,2023,"Zhengwei Yang, Meng Lin, Xian Zhong, Yu Wu, Zheng Wang","Entangled representation of clothing and identity (ID)-intrinsic clues are potentially concomitant in conventional person Re-IDentification (ReID). Nevertheless, eliminating the negative impact of clothing on ID remains challenging due to the lack of theory and the difficulty of isolating the exact implications. In this paper, a causality-based Auto-Intervention Model, referred to as AIM, is first proposed to mitigate clothing bias for robust cloth-changing person ReID (CC-ReID). Specifically, we analyze the effect of clothing on the model inference and adopt a dual-branch model to simulate causal intervention. Progressively, clothing bias is eliminated automatically with model training. AIM is encouraged to learn more discriminative ID clues that are free from clothing bias. Extensive experiments on two standard CC-ReID datasets demonstrate the superiority of the proposed AIM over other state-of-the-art methods.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_Good_Is_Bad_Causality_Inspired_Cloth-Debiasing_for_Cloth-Changing_Person_Re-Identification_CVPR_2023_paper.pdf,,3,本文提出了一种基于因果关系的自动干预模型AIM，旨在消除传统人物再识别中服装对身份识别的负面影响。通过分析服装对模型推断的影响，采用双分支模型模拟因果干预，逐步消除服装偏差。AIM能够学习更具有辨别力的身份线索，不受服装偏差 的影响。在两个标准的服装变换人物再识别数据集上进行了广泛的实验，证明了AIM相对于其他最先进的方法的优越性。
17,Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences Between Pretrained Generative Models,CVPR,2023,"Matthew L. Olson, Shusen Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Peer-Timo Bremer, Weng-Keen Wong","Generative Adversarial Networks (GANs) are notoriously difficult to train especially for complex distributions and with limited data. This has driven the need for interpretable tools to audit trained networks, for example, to identify biases or ensure fairness. Existing GAN audit tools are restricted to coarse-grained, model-data comparisons based on summary statistics such as FID or recall. In this paper, we propose an alternative approach that compares a newly developed GAN against a prior baseline. To this end, we introduce Cross-GAN Auditing (xGA) that, given an established ""reference"" GAN and a newly proposed ""client"" GAN, jointly identifies semantic attributes that are either common across both GANs, novel to the client GAN, or missing from the client GAN. This provides both users and model developers an intuitive assessment of similarity and differences between GANs. We introduce novel metrics to evaluate attribute-based GAN auditing approaches and use these metrics to demonstrate quantitatively that xGA outperforms baseline approaches. We also include qualitative results that illustrate the common, novel and missing attributes identified by xGA from GANs trained on a variety of image datasets.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Olson_Cross-GAN_Auditing_Unsupervised_Identification_of_Attribute_Level_Similarities_and_Differences_CVPR_2023_paper.pdf,,3,本文介绍了一种新的生成对抗网络（GAN）审计方法，名为Cross-GAN Auditing（xGA）。与现有的基于摘要统计量的粗略模型-数据比较方法不同，xGA将新开发的GAN与基准GAN进行比较，共同识别在两个GAN之间共同的语义属性、客户端GAN中新颖的 语义属性以及客户端GAN中缺失的语义属性。这种方法为用户和模型开发人员提供了直观的GAN相似性和差异性评估。本文还引入了新的指标来评估基于属性的GAN审计方法，并使用这些指标定量地证明xGA优于基准方法。同时，本文还包括了定性结果，展示了xGA从各种图像数据集训练的GAN中识别出的共同、新颖和缺失的属性。
18,TOPLight: Lightweight Neural Networks With Task-Oriented Pretraining for Visible-Infrared Recognition,CVPR,2023,"Hao Yu, Xu Cheng, Wei Peng","Visible-infrared recognition (VI recognition) is a challenging task due to the enormous visual difference across heterogeneous images. Most existing works achieve promising results by transfer learning, such as pretraining on the ImageNet, based on advanced neural architectures like ResNet and ViT. However, such methods ignore the negative influence of the pretrained colour prior knowledge, as well as their heavy computational burden makes them hard to deploy in actual scenarios with limited resources. In this paper, we propose a novel task-oriented pretrained lightweight neural network (TOPLight) for VI recognition. Specifically, the TOPLight method simulates the domain conflict and sample variations with the proposed fake domain loss in the pretraining stage, which guides the network to learn how to handle those difficulties, such that a more general modality-shared feature representation is learned for the heterogeneous images. Moreover, an effective fine-grained dependency reconstruction module (FDR) is developed to discover substantial pattern dependencies shared in two modalities. Extensive experiments on VI person re-identification and VI face recognition datasets demonstrate the superiority of the proposed TOPLight, which significantly outperforms the current state of the arts while demanding fewer computational resources.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_TOPLight_Lightweight_Neural_Networks_With_Task-Oriented_Pretraining_for_Visible-Infrared_Recognition_CVPR_2023_paper.pdf,,3,本文提出了一种新的面向任务的预训练轻量级神经网络（TOPLight）用于可见光红外识别（VI recognition）。TOPLight方法在预训练阶段使用虚假域损失来模拟领域冲突和样本变化，引导网络学习如何处理这些困难，从而学习到更通用的模态共享 特征表示。此外，还开发了一种有效的细粒度依赖重构模块（FDR），用于发现两种模态中共享的重要模式依赖关系。在可见光人员再识别和可见光人脸识别数据集上进行了广泛的实验，结果表明，所提出的TOPLight方法明显优于当前的艺术水平，同时需要更少的计算资源。
19,Deep Semi-Supervised Metric Learning With Mixed Label Propagation,CVPR,2023,"Furen Zhuang, Pierre Moulin","Metric learning requires the identification of far-apart similar pairs and close dissimilar pairs during training, and this is difficult to achieve with unlabeled data because pairs are typically assumed to be similar if they are close. We present a novel metric learning method which circumvents this issue by identifying hard negative pairs as those which obtain dissimilar labels via label propagation (LP), when the edge linking the pair of data is removed in the affinity matrix. In so doing, the negative pairs can be identified despite their proximity, and we are able to utilize this information to significantly improve LP's ability to identify far-apart positive pairs and close negative pairs. This results in a considerable improvement in semi-supervised metric learning performance as evidenced by recall, precision and Normalized Mutual Information (NMI) performance metrics on Content-based Information Retrieval (CBIR) applications.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhuang_Deep_Semi-Supervised_Metric_Learning_With_Mixed_Label_Propagation_CVPR_2023_paper.pdf,,3,本文介绍了一种新的度量学习方法，通过识别在标签传播过程中获得不相似标签的困难负样本对来解决无标签数据中相似对和不相似对的识别问题。通过这种方法，即使负样本对非常接近，也能够识别出来，并且能够利用这些信息显著提高标签传播 算法在内容检索应用中识别远距离正样本和接近负样本的能力，从而显著提高半监督度量学习的性能。这一方法的有效性通过召回率、精确度和标准化互信息等性能指标在内容检索应用中得到了证明。
20,Jedi: Entropy-Based Localization and Removal of Adversarial Patches,CVPR,2023,"Bilel Tarchoun, Anouar Ben Khalifa, Mohamed Ali Mahjoub, Nael Abu-Ghazaleh, Ihsen Alouani","Real-world adversarial physical patches were recently shown to be successful in compromising state-of-the-art models in a variety of computer vision applications. The most promising defenses that are based on either input gradient or features analyses have been shown to be compromised by recent GAN-based adaptive attacks that generate realistic/naturalistic patches. In this paper, we propose Jedi, a new defense against adversarial patches that is resilient to realistic patch attacks, and also improves detection and recovery compared to the state of the art. Jedi leverages two new ideas: (1) it improves the identification of potential patch regions using entropy analysis: we show that the entropy of adversarial patches is high, even in naturalistic patches; and (2) it improves the localization of adversarial patches, using an autoencoder that is able to complete patch regions and filter out normal regions with high entropy that are not part of a patch. Jedi achieves high precision adversarial patch localization, which we show is critical to successfully repair the images. Since Jedi relies on an input entropy analysis, it is model-agnostic, and can be applied on pre-trained off-the-shelf models without changes to the training or inference of the protected models. Jedi detects on average 90% of adversarial patches across different benchmarks and recovers up to 94% of successful patch attacks (Compared to 75% and 65% for LGS and Jujutsu, respectively). Jedi is also able to continue detection even in the presence of adaptive realistic patches that are able to fool other defenses.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Tarchoun_Jedi_Entropy-Based_Localization_and_Removal_of_Adversarial_Patches_CVPR_2023_paper.pdf,,3,本文介绍了一种新的防御方法Jedi，用于对抗现实世界中的对抗性物理贴片攻击。该方法利用熵分析和自编码器技术，提高了对潜在贴片区域的识别和定位能力，并且在检测和修复方面比现有技术有所提高。Jedi可以在不改变受保护模型的训练或推 理的情况下，应用于预训练的现成模型。实验结果表明，Jedi可以在不同的基准测试中检测到平均90%的对抗性贴片，并且可以恢复高达94%的成功攻击（相比之下，LGS和Jujutsu分别为75%和65%）。Jedi还能够在存在能够欺骗其他防御措施的自适应现实贴片的情况下继 续检测。
21,Diverse Embedding Expansion Network and Low-Light Cross-Modality Benchmark for Visible-Infrared Person Re-Identification,CVPR,2023,"Yukang Zhang, Hanzi Wang","For the visible-infrared person re-identification (VIReID) task, one of the major challenges is the modality gaps between visible (VIS) and infrared (IR) images. However, the training samples are usually limited, while the modality gaps are too large, which leads that the existing methods cannot effectively mine diverse cross-modality clues. To handle this limitation, we propose a novel augmentation network in the embedding space, called diverse embedding expansion network (DEEN). The proposed DEEN can effectively generate diverse embeddings to learn the informative feature representations and reduce the modality discrepancy between the VIS and IR images. Moreover, the VIReID model may be seriously affected by drastic illumination changes, while all the existing VIReID datasets are captured under sufficient illumination without significant light changes. Thus, we provide a low-light cross-modality (LLCM) dataset, which contains 46,767 bounding boxes of 1,064 identities captured by 9 RGB/IR cameras. Extensive experiments on the SYSU-MM01, RegDB and LLCM datasets show the superiority of the proposed DEEN over several other state-of-the-art methods. The code and dataset are released at: https://github.com/ZYK100/LLCM",,https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Diverse_Embedding_Expansion_Network_and_Low-Light_Cross-Modality_Benchmark_for_Visible-Infrared_CVPR_2023_paper.pdf,https://github.com/ZYK100/LLCM,3,本文介绍了可见光-红外人物再识别（VIReID）任务中的一个主要挑战：可见光（VIS）和红外（IR）图像之间的模态差距。然而，训练样本通常很有限，而模态差距太大，这导致现有方法无法有效地挖掘多样的跨模态线索。为了解决这个限制，作者 提出了一种新颖的嵌入空间增强网络，称为多样嵌入扩展网络（DEEN）。所提出的DEEN可以有效地生成多样的嵌入，学习信息丰富的特征表示，并减少VIS和IR图像之间的模态差异。此外，VIReID模型可能会受到剧烈照明变化的严重影响，而所有现有的VIReID数据集都是在充足照明下捕获的，没有显著的光线变化。因此，作者提供了一个低光交叉模态（LL
22,Continuous Sign Language Recognition With Correlation Network,CVPR,2023,"Lianyu Hu, Liqing Gao, Zekang Liu, Wei Feng","Human body trajectories are a salient cue to identify actions in video. Such body trajectories are mainly conveyed by hands and face across consecutive frames in sign language. However, current methods in continuous sign language recognition(CSLR) usually process frames independently to capture frame-wise features, thus failing to capture cross-frame trajectories to effectively identify a sign. To handle this limitation, we propose correlation network (CorrNet) to explicitly leverage body trajectories across frames to identify signs. In specific, an identification module is first presented to emphasize informative regions in each frame that are beneficial in expressing a sign. A correlation module is then proposed to dynamically compute correlation maps between current frame and adjacent neighbors to capture cross-frame trajectories. As a result, the generated features are able to gain an overview of local temporal movements to identify a sign. Thanks to its special attention on body trajectories, CorrNet achieves new state-of-the-art accuracy on four large-scale datasets, PHOENIX14, PHOENIX14-T, CSL-Daily, and CSL. A comprehensive comparison between CorrNet and previous spatial-temporal reasoning methods verifies its effectiveness. Visualizations are given to demonstrate the effects of CorrNet on emphasizing human body trajectories across adjacent frames.",,https://openaccess.thecvf.com/content/CVPR2023/papers/Hu_Continuous_Sign_Language_Recognition_With_Correlation_Network_CVPR_2023_paper.pdf,,3,本文提出了一种基于相关网络（CorrNet）的连续手语识别方法，以充分利用手语中手和面部的运动轨迹。传统的连续手语识别方法通常独立处理每一帧以捕捉帧级特征，无法有效地捕捉跨帧轨迹以识别手语。为了解决这个问题，本文提出了一个识别模块和一个相关模块，分别用于强调每一帧中有益于表达手语的信息区域和动态计算当前帧和相邻帧之间的相关性图以捕捉跨帧轨迹。由于CorrNet特别关注人体轨迹，因此在PHOENIX14、PHOENIX14-T、CSL-Daily和CSL四个大型数据集上取得了新的最高准确率。本文还通过可视化效果展示了CorrNet在强调相邻帧中的人体轨迹方面的效果。
